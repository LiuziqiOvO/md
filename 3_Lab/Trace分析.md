# Twitter Trace分析

A large scale analysis of hundreds of in-memory cache clusters at Twitter

Juncheng Yang  CMU 

https://www.usenix.org/system/files/osdi20-yang.pdf

对微服务架构下，单租户、单层内存缓存的负载分析。

文章的主要发现包括：

1. 内存中缓存不仅仅服务于读重的工作负载，写重的工作负载非常普遍，超过35%的153个缓存集群存在这种情况。
2. TTL在内存中缓存中必须被考虑，因为它限制了有效的（未过期的）工作集大小。有效地从缓存中移除过期对象需要优先于缓存驱逐。
3. 内存中缓存工作负载遵循近似Zipfian流行度分布，有时具有非常高的偏斜。显示出最大偏差的工作负载往往是写重的工作负载。

这项研究提供了对生产内存中缓存系统的详细新视角，同时揭示了一些不符合传统观念和常用假设的令人惊讶的方面。研究者们还提供了Twitter缓存工作负载的追踪数据，以便研究社区使用 。





## **摘要**

此工作中通过收集 Twitter 上 153 个内存中缓存集群的生产追踪，筛选超过 80 TB 的数据。并有时在它们背后的业务逻辑背景下解释这些工作负载。

我们进行全面分析，根据

- 流量模式、
- 生存时间（TTL）
- 流行度分布
- 大小分布

来描述缓存工作负载。

不同工作负载的细粒度视图揭示了用例的多样性：许多工作负载比之前显示的更加写重或更加偏斜，有些显示出独特的时间模式。我们还观察到 TTL 是缓存工作集的一个重要且有时是决定性的参数。我们的模拟表明，生产缓存中理想的替换策略可能是令人惊讶的，例如，FIFO 对于大量工作负载效果最好。



## 1 引言

像 Memcached 和 Redis 这样的内存缓存系统被web程序广泛使用，以减少对存储的访问并避免重复计算。

但大家对负载的理解仍有不足：

- 首先，缺乏涵盖当今生产系统中广泛用例的全面研究。
- 其次，自之前工作发表以来，内存中缓存使用出现了新趋势。
- 第三，内存中缓存的一些方面在现有研究中受到的关注较少，但被实践者认为是关键的。例如，TTL 是配置内存中缓存的一个重要方面，但在研究中大多被忽视了。
- 最后，缺乏开源的内存缓存追踪。研究人员不得不依赖存储缓存追踪、键值数据库基准测试或合成工作负载来评估内存中缓存系统。这些来源要么具有不同的特征，要么没有捕捉到生产内存中缓存工作负载的所有特征。例如，键值数据库基准测试和合成工作负载没有考虑对象大小分布随时间的变化，这影响了内存中缓存系统的未命中率和吞吐量。

在这项工作中，我们通过收集和分析 Twitter 上 153 个 Twemcache 集群的工作负载追踪。这是第一个研究超过 100 个不同缓存工作负载，涵盖广泛的用例的工作。我们相信这些工作负载代表了社交媒体公司以及更广泛的缓存使用情况，并希望为未来的缓存系统设计提供基础。以下是我们发现的总结：

1. 内存中缓存不仅仅服务于读重的工作负载，写重（定义为写比例 > 30%）的工作负载非常常见，在我们研究的 153 个缓存集群中超过 35%。
2. TTL 必须在内存中缓存中被考虑，因为它限制了有效的（未过期的）工作集大小。有效地从缓存中移除过期对象需要优先于缓存驱逐。
3. 内存中缓存工作负载遵循近似 Zipfian 流行度分布，有时具有非常高的偏斜。显示出最大偏差的工作负载往往是写重的工作负载。
4. 对象大小分布并不随时间静止。有些工作负载显示出昼夜模式，并经历突然的、短暂的变化，这对基于 slab 的缓存系统（如 Memcached）提出了挑战。
5. 在合理的缓存大小下，FIFO 通常表现出与 LRU 相似的性能，而 LRU 通常只在缓存大小严重受限时才表现出优势。 这些发现为生产环境中的内存中缓存系统提供了详细的新视角，同时揭示了一些不符合传统观念和常用假设的令人惊讶的方面。

## 2 服务架构和缓存

(微服务架构下的内存缓存)

### 架构

Twitter 从 2011 年开始向微服务。大约在同一时间，Twitter 开始开发其容器解决方案以支持即将到来的服务浪潮。到了 2020 年，实时服务堆栈主要是面向服务的，有数百个服务在生产环境中的容器内运行。(缓存集群都是容器化的。)

在 Twitter，缓存是一项托管服务，新的缓存集群根据请求半自动地配置为旁路缓存。生产环境中部署了两种内存中缓存解决方案：Twemcache（Memcached 的一个分支）是一个提供高吞吐量和低延迟的键值缓存；另一种解决方案是 Nighthawk，它基于 Redis 并支持丰富的数据结构和复制以实现数据可用性。在这项工作中，我们关注 Twemcache，因为它承担了大部分缓存流量。

Twitter 上的缓存集群被认为是基于请求它们的服务团队的**单租户**。这种设置对工作负载分析非常有利，因为它允许我们标记用例，收集追踪，并单独研究工作负载的属性。多租户设置将使类似的研究变得极其困难，因为研究人员必须从混合中梳理出各个工作负载，并以某种方式将它们与它们的用例联系起来。此外，由于流量低，较小但独特的工作负载很容易被忽视或误判。

与其他缓存集群部署不同，例如社交图缓存或 CDN 缓存，Twemcache 主要被部署为单层缓存，这使我们能够直接从客户端分析请求，而不会被其他缓存过滤。先前的工作已经表明，**分层对缓存工作负载的属性有影响**，例如流行度分布。这种单租户、单层设计为我们提供了研究工作负载属性的完美机会。

###  Twemcache 配置

截至目前，每个数据中心大约有 200 个 Twemcache 集群。Twemcache 容器高度同质化，通常很小，单个主机可以运行很多容器。

**集群实例的计算**：每个缓存集群的实例数量是根据用户输入计算得出的，包括吞吐量、估计的数据集大小和容错性。首先确定正确的瓶颈，然后应用其他约束（如支持的连接数）来自动计算每个集群的实例数量。生产缓存集群的大小从20个实例到数千个实例不等。

### Twemcache概述

**基于slab的内存管理**：Twemcache是从Memcached的一个早期版本分支出来的，并添加了一些定制功能。Twemcache经常存储小的、可变大小的对象，大小范围从几字节到几十KB。在这种情况下，按需堆内存分配器（如ptmalloc、jemalloc）可能会导致大量且无限制的外部内存碎片，这在生产环境中是非常不受欢迎的，尤其是在使用较小容器时。为了避免这种情况，Twemcache继承了Memcached的基于slab的内存管理（见图1）。内存被分配为固定大小的块，称为slabs，默认大小为1MB。每个slab再被均匀划分为更小的块，称为items。每个slab的类别决定了其items的大小。默认情况下，Twemcache的物品大小从可配置的最小值（默认为88字节）增长到接近整个slab的大小。增长通常是指数级的，由一个称为增长因子的浮点数控制（默认为1:25），尽管Twemcache也允许精确配置特定的物品大小。更高的slab类别对应更大的物品。一个对象被映射到最适合它的slab类别，包括元数据。在Twemcache中，每个对象的元数据是49字节。默认情况下，一个类别12的slab有891个物品，每个物品大小为1176字节，每个物品存储最多1127字节的键加值。基于slab的分配器消除了外部内存碎片，但代价是有限内部内存碎片。

> 以Slab为管理单位，默认为1MB。
>
> 每个slab再被均匀划分为更小的块，称为**items**。每个slab的类别决定了其items的大小。默认情况下，Twemcache的物品大小从可配置的最小值（默认为88字节）增长到接近整个slab的大小。

**基于slab的缓存驱逐**：要存储一个新对象，Twemcache首先根据对象大小计算slab类别。如果在这个类别的slab中至少有一个空闲物品，Twemcache就使用这个空闲物品。否则，Twemcache尝试为此类别分配一个新的slab。当内存满了，就需要进行slab驱逐以便分配。 一些缓存系统，如Memcached，主要执行项目级别的驱逐，这发生在与新对象相同的slab类别中。Memcached使用每个slab类别的近似LRU队列来跟踪和驱逐最近最少使用的物品。只要对象大小分布保持静态，这种方法就有效。然而，实际情况往往并非如此。例如，如果所有键都以小值开始，随着时间的推移而增长，新的写入最终将需要在更高的slab类别中存储对象。然而，如果此时所有内存都已分配，实际上将没有内存可以分配。这个问题被称为slab钙化，并在第4.6.2节中进一步探讨。Memcached开发了一系列启发式方法来在slab类别之间移动内存，但已被证明是非最优的[10,11,17,46]和容易出错的[9]。 为了避免slab钙化，Twemcache只使用slab驱逐（见图1）。这允许被驱逐的slab转变为任何其他slab类别。选择驱逐的slab有三种方法：随机选择一个slab（随机slab）、选择最近最少使用的slab（slabLRU）和选择最近创建的slab（slabLRC）。除了避免slab钙化，仅slab驱逐还比Memcached减少了两个指向对象元数据的指针。我们在第6节进一步比较了对象驱逐和slab驱逐。

### Cache使用场景

在Twitter，Twemcache的主要用途通常被认为有三种：存储缓存、计算缓存和瞬态数据缓存。需要注意的是，这三种类别之间没有严格的界限，生产集群也没有明确的标签。因此，下面给出的百分比是基于我们对每个缓存集群及其相应应用的理解所做的粗略估计。

**2.4.1 存储缓存**

使用缓存来促进从存储中读取数据是最常见的用例。后端存储（如数据库）通常比内存缓存有更长的延迟和更低的带宽。因此，缓存这些对象可以减少访问延迟，提高吞吐量，并保护后端免受过多的读流量。这个用例在研究中受到了最多的关注。为了减少未命中率，已经投入了多项努力，包括重新设计以适应更大工作集的存储设备、改善负载均衡和提高吞吐量。

如图2所示，尽管只有30%的集群属于这一类，但它们占Twemcache处理请求的65%，占总DRAM使用量的60%，以及所有分配的CPU核心的50%。

**2.4.2 计算缓存**

计算缓存并不是新概念——使用DRAM缓存查询结果自二十多年前就开始被研究和使用。随着实时流处理和机器学习（ML）变得越来越流行，越来越多的缓存集群被用于缓存与计算相关的数据，如特征、ML预测的中间和最终结果，以及所谓的对象水合——用额外的数据填充对象，这通常结合了存储访问和计算。

总的来说，计算缓存占所有Twemcache集群的50%，在集群数量上占26%，在请求率、缓存大小和CPU核心上分别占31%和40%。

**2.4.3 无后端存储的瞬态数据缓存**

第三种典型的缓存使用是围绕只存在于缓存中的对象，通常持续时间很短。它不是严格意义上的缓存，因此受到了很少的关注。尽管如此，在内存缓存通常是满足这类用例的性能和可扩展性要求的唯一生产解决方案。虽然数据丢失仍然是不可取的，但这些用例非常重视速度，并且足够容忍偶尔的数据丢失，以至于可以在没有后备方案的情况下工作。

一些显著的例子包括速率限制器、去重缓存和负结果缓存。速率限制器是与用户活动相关的计数器。它们跟踪并限制给定时间窗口内的用户请求，并防止拒绝服务攻击。去重缓存是速率限制器的一种特殊情况，其中限制是1。负结果缓存存储来自较大数据库的键，这些键在较小、稀疏填充的数据库中已知是未命中的。这些缓存通过负结果截断大多数查询，并大幅减少针对较小数据库的流量。

在我们的测量中，20%的Twemcache集群属于这一类。它们的请求率和缓存大小分别占所有Twemcache请求率和缓存大小的9%和8%，同时占所有Twemcache集群CPU核心的10%。

## 3 方法(收集数据)

这篇文章的第3节“方法论”描述了作者们如何收集和分析Twemcache的日志数据。以下是该部分的总结：

### 3.1 日志收集

Twemcache内置了一个非阻塞的请求日志记录工具，名为klog，它能够在生产环境中跟上设计吞吐量。虽然默认情况下它只记录每100个请求中的一个，但作者们动态地将抽样比例改为100%，并从每个Twemcache集群的两个实例中收集了一周长的未抽样跟踪数据。收集未抽样的跟踪数据可以避免因抽样而得出可能的有偏结论。此外，作者们选择从两个实例而不是一个实例收集跟踪数据，以防止在日志收集期间可能发生的缓存故障，并比较不同实例之间的结果以提高分析的准确性。除非发生缓存故障，否则这两个实例没有重叠的键。

### 3.2 日志概览

作者们从153个Twemcache集群的306个实例中收集了大约7000亿个请求（原始文件大小为80TB），这些集群包括在收集时每个实例的请求率超过1000次每秒（QPS）的所有集群。为了简化分析和展示，作者们专注于54个最大的缓存，这些缓存占总QPS的90%和分配内存的76%。在接下来的章节中，作者们使用Twemcache工作负载来指代这54个Twemcache集群的工作负载。尽管作者们只呈现了这54个缓存的结果，但他们确实对较小的缓存进行了相同的分析，而这些较小的缓存并没有改变他们的结论。



## 4 生产环境和负载分析

## 4.1 未命中率（Miss Ratio）

未命中率是衡量缓存有效性的关键指标之一。生产环境中的内存缓存通常以低未命中率和较小的未命中率变化运行。文章展示了按请求率排名的前十个Twemcache集群的未命中率（图3a），其中点表示一周内的平均未命中率，误差条显示最小和最大未命中率。十个Twemcache集群中有八个的未命中率低于5%，其中六个的未命中率接近或低于1%。唯一的例外是一个写入密集型缓存集群，其未命中率约为70%（关于写入密集型工作负载的详细信息见第4.3.2节）。与CDN缓存相比，内存缓存通常具有更低的未命中率。

除了低未命中率外，未命中率的稳定性也非常重要。在生产环境中，最高的未命中率（和请求率）决定了后端的QPS要求。因此，一个大部分时间未命中率低但有时高的缓存，不如一个未命中率稍高但稳定的缓存有用。图3b显示了不同缓存在一周内miss ratio的最大值与最小值的比率，我们观察到大多数缓存的这个比率低于1.5。此外，通常具有较大比率的缓存通常具有非常低的未命中率。

低未命中率和高稳定性通常表明生产缓存的有效性。然而，极低的未命中率往往不够健壮，这意味着相应的后端需要更多的预留空间。此外，缓存维护和故障成为极低未命中率缓存的主要中断源。这些因素的结合表明，缓存通常在减少读流量或后端需要预留的流量方面有一个限制。

## 4.2 请求率和热键

与之前观察到的类似，请求率显示出日间模式（图4）。此外，请求率的峰值也很常见，因为缓存是对前端服务和最终用户任何变化的第一反应者。

当请求率峰值发生时，一个普遍的看法是热键导致峰值。确实，负载峰值通常是热键的结果。然而，我们注意到情况并非总是如此。如图4所示，在请求率（顶部蓝线）峰值时，同一时间间隔内访问的对象数量（底部红线）也出现峰值，表明峰值是由热键以外的因素触发的。这些因素包括客户端重试请求、外部流量激增、扫描式访问和周期性任务。

除了请求率峰值外，缓存通常还显示出其他不规则性。例如，在第4.6.2节中，我们展示了对象大小分布的突然变化很常见。这些不规则性可能因多种原因而发生。例如，用户因社交事件而改变行为，前端服务添加了新功能（或错误），或者启动了内部负载测试。

作为基础设施的关键组成部分，缓存阻止了大部分请求击中后端，它们应该被设计为能够容忍这些工作负载变化以吸收影响。

## 4.3 操作类型

Twemcache支持十一种不同的操作，其中get和set迄今为止是最常用的。此外，在Twitter上，写入密集型缓存工作负载非常常见。

**4.3.1 相对使用比较**

我们从Twemcache工作负载使用的操作开始。Twemcache支持十一种操作：get、gets、set、add、cas（检查并设置）、replace、append、prepend、delete、incr和decr。如图5a所示，get和set是两个最常用的操作，平均get比率接近90%，表明大多数缓存正在提供读密集型工作负载。除了get和set，gets、add、cas、delete、incr操作也在Twemcache集群中频繁使用。然而，与get和set相比，这些操作通常占所有请求的较小百分比。尽管如此，这些操作在内存缓存中扮演着重要角色。因此，正如Memcached的作者所建议的，不应忽视它们。

**4.3.2 写入比率**

尽管大多数缓存以读为主，但图5a显示get和set比率在缓存之间有很大的范围。我们定义一个工作负载为写入密集型，如果set、add、cas、replace、append、prepend、incr和decr操作的百分比总和超过30%。图5b显示了缓存之间的写入比率分布。超过35%的所有Twemcache集群都是写入密集型的，超过20%的集群写入比率高于50%。换句话说，除了众所周知的提供读密集型工作负载的用例外，还有相当数量的Twemcache集群用于提供写入密集型工作负载。我们下面识别了写入密集型缓存的主要用例。

- **频繁更新的数据**：这类缓存大多属于计算缓存或瞬态数据（第2.4.2节和2.4.3节）。更新在缓存中累积，然后才持久化，或者键最终过期。
- **机会主义预计算**：一些服务持续为潜在消费生成数据，无论是自己还是其他服务。一个例子是存储最近用户活动的缓存，当查询请求特定用户的最近事件时，缓存的数据被读取。许多服务选择不按需获取相关数据，而是为更广泛的用户群体机会主义地预计算它们。这是可行的，因为预计算通常有有界成本，并且作为交换，读查询可以由预计算结果部分或完全快速满足。由于这主要是为了用户体验的权衡，这类缓存下的对象重用较少。因此，写入比率通常较高（>80%），对象访问（读+写）频率通常较低。在一个案例中，我们看到一个集群的平均对象频率接近1。



















#  Ali BlockTrace 分析

An In-Depth Analysis of Cloud Block Storage Workloads in Large-Scale Production

